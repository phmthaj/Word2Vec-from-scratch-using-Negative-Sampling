{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51bbfdeb-8aa4-4590-98e9-1e6d74409751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "973599c3-395a-4ea7-9aaf-ef70fc787be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the morning sky was pale and quiet.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>she brewed coffee while the cat watched the wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i left my keys on the kitchen table again.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the bus arrived late because of heavy rain.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>students lined up outside the library before o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>we rolled back and wrote a postmortem.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>the cafe offered quiet seats with strong wifi.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>rain tapped lightly on the window glass.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>a street musician played jazz under the bridge.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>night markets glowed with neon and steam.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentence\n",
       "0                 the morning sky was pale and quiet.\n",
       "1   she brewed coffee while the cat watched the wi...\n",
       "2          i left my keys on the kitchen table again.\n",
       "3         the bus arrived late because of heavy rain.\n",
       "4   students lined up outside the library before o...\n",
       "..                                                ...\n",
       "56             we rolled back and wrote a postmortem.\n",
       "57     the cafe offered quiet seats with strong wifi.\n",
       "58           rain tapped lightly on the window glass.\n",
       "59    a street musician played jazz under the bridge.\n",
       "60          night markets glowed with neon and steam.\n",
       "\n",
       "[61 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('text_corpus.txt',names = ['sentence'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faf322f5-d270-45aa-b229-a437b7ac112b",
   "metadata": {},
   "outputs": [],
   "source": [
    "value = df.iloc[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d2134df-a704-4f85-b14a-92e3ddc2d20f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'she brewed coffee while the cat watched the window.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1083058a-fb50-4cb5-8d7b-d2486fde9d46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the morning sky was pale and quiet.',\n",
       " 'she brewed coffee while the cat watched the window.',\n",
       " 'i left my keys on the kitchen table again.',\n",
       " 'the bus arrived late because of heavy rain.',\n",
       " 'students lined up outside the library before opening.',\n",
       " 'the teacher wrote three formulas on the board.',\n",
       " 'we measured the distance with a cheap ruler.',\n",
       " 'a small error slipped into the final report.',\n",
       " 'the startup pitched their idea to five investors.',\n",
       " 'users complained about the app crashing at midnight.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = []\n",
    "for i in range(len(df)):\n",
    "    value = df.iloc[i,0]\n",
    "    sentences.append(value)\n",
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fdae727-cdfa-46e3-a4ee-9765e9300a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(corpus):\n",
    "    words = []\n",
    "    for s in corpus:\n",
    "        for w in s.split():\n",
    "            if w not in words:\n",
    "                words.append(w)\n",
    "    return words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5f7015a-8c15-4695-af18-f9cb4573daed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'morning',\n",
       " 'sky',\n",
       " 'was',\n",
       " 'pale',\n",
       " 'and',\n",
       " 'quiet.',\n",
       " 'she',\n",
       " 'brewed',\n",
       " 'coffee']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = preprocessing(sentences)\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7a6fa57-f03a-45fe-af07-81c7539d41e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexing_word(words):\n",
    "    word_to_idx = {word:idx for idx,word in enumerate(words)}\n",
    "    idx_to_word = {idx:word for idx,word in enumerate(words)}\n",
    "    return word_to_idx, idx_to_word\n",
    "\n",
    "word_to_idx, idx_to_word = indexing_word(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94b68a9e-47e2-41ea-af1f-ccb202214320",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = 2\n",
    "embedding_dim = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "232b3c67-175a-4802-aeee-1b3b327ee6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_data(sentences, words, word_to_idx, idx_to_word):\n",
    "    data = []\n",
    "    for s in sentences:\n",
    "        sen = s.split()\n",
    "        m = len(sen)\n",
    "        for i in range(context_size, m - context_size):\n",
    "            target_word = sen[i]\n",
    "            context_words = []\n",
    "            for j in range(1,context_size+1):\n",
    "                con_w_1 = sen[i-j]\n",
    "                con_w_2 = sen[i+j]\n",
    "                if con_w_1 not in context_words:\n",
    "                    context_words.append(con_w_1)\n",
    "                if con_w_2 not in context_words:\n",
    "                    context_words.append(con_w_2)\n",
    "            for con_w in context_words:\n",
    "                if con_w not in data:\n",
    "                    data.append((target_word,con_w))\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2d7a0f2-86a9-4354-93ca-b707bebf00f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sky', 'morning'),\n",
       " ('sky', 'was'),\n",
       " ('sky', 'the'),\n",
       " ('sky', 'pale'),\n",
       " ('was', 'sky'),\n",
       " ('was', 'pale'),\n",
       " ('was', 'morning'),\n",
       " ('was', 'and'),\n",
       " ('pale', 'was'),\n",
       " ('pale', 'and')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = compile_data(sentences, words, word_to_idx, idx_to_word)\n",
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc2372a5-74cf-4713-82ac-a02d1d65ff57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c66ae5d-5539-42f8-8a91-424660ea673b",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_data = [(word_to_idx[w1], word_to_idx[w2]) for (w1, w2) in data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efcc2cb7-4be5-4324-b421-a9ac5e17bfac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 1),\n",
       " (2, 3),\n",
       " (2, 0),\n",
       " (2, 4),\n",
       " (3, 2),\n",
       " (3, 4),\n",
       " (3, 1),\n",
       " (3, 5),\n",
       " (4, 3),\n",
       " (4, 5)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25981017-5329-4ab6-9620-cb447e85037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class Word2VecDataset(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        self.pairs = pairs   \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.pairs[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc97bdf5-75b3-45dd-a5c4-e73b6ba65f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Word2VecDataset(indexed_data)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2679dd54-d917-4c14-89e4-366907d853c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Word2VecDataset object at 0x00000193B4B76050>\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f13eb3c-5a4b-40f0-b511-0a288bc90a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "341"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_size = len(words)\n",
    "word_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "389265d8-660d-40c5-8c8f-a22471df452a",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_nums = 8\n",
    "indexed_set = set(indexed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "894cf456-6e13-4834-99d8-b9bf3493a9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_negative_sampling(center_w, context_w, word_size, negative_nums, indexed_set):\n",
    "    neg_samples = []\n",
    "    while len(neg_samples) < negative_nums:\n",
    "        neg = np.random.randint(0, word_size)\n",
    "        if neg != context_w and (center_w, neg) not in indexed_set:\n",
    "            neg_samples.append(neg)\n",
    "    return neg_samples\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b39959c4-aa81-48a1-bfe6-e742d1e10512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class NegativeSamplingModel(nn.Module):\n",
    "    def __init__(self, word_size, embedding_dim):\n",
    "        super(NegativeSamplingModel, self).__init__()\n",
    "        self.word_size = word_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.Wv_embedding = nn.Embedding(word_size, embedding_dim)      # Wv la ma tran V x dim, Wv chua vector v_w\n",
    "        self.Wu_embedding = nn.Embedding(word_size, embedding_dim)      # Wu la ma tran V x dim, Wu chua vector u_w\n",
    "        self.log_sigmoid = nn.LogSigmoid()\n",
    "    def forward(self, center_word, context_word, negative_samples):\n",
    "        center = self.Wv_embedding(center_word)         # Dang 32 x dim do 1 batch = 32\n",
    "        context = self.Wu_embedding(context_word)       # Dang 32 x dim do 1 batch = 32\n",
    "        neg = self.Wu_embedding(negative_samples)       # Lay K negative sampling tu ma tran V x dim => duoc ma tran k x dim, them 1 batch = 32 => 32 x k x dim\n",
    "\n",
    "        pos_loss = self.log_sigmoid(torch.sum(center*context, dim=1))\n",
    "        neg_score = torch.sum(neg * center.unsqueeze(1), dim=2)     # size 32 x k x dim va size 32 x 1 x dim => 32 x k x dim, cong theo dim=2 => 32xk\n",
    "        neg_loss = self.log_sigmoid(-neg_score)                      # size 32 x k\n",
    "        neg_loss = torch.sum(neg_loss, dim=1)                        # size 32\n",
    "        loss = -(pos_loss + neg_loss).mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21797a9f-2479-4d30-9e4d-2ea82e3852d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "model = NegativeSamplingModel(word_size, embedding_dim)\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e828d89-efa9-4a07-8d18-d7e3594d1239",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c531293-db4c-42eb-a225-dde0cd4ae079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Loss: 1171.0338\n",
      "Epoch 21/1000, Loss: 842.5060\n",
      "Epoch 41/1000, Loss: 677.7638\n",
      "Epoch 61/1000, Loss: 522.7814\n",
      "Epoch 81/1000, Loss: 363.4334\n",
      "Epoch 101/1000, Loss: 228.1483\n",
      "Epoch 121/1000, Loss: 136.7229\n",
      "Epoch 141/1000, Loss: 79.1784\n",
      "Epoch 161/1000, Loss: 43.3326\n",
      "Epoch 181/1000, Loss: 21.6457\n",
      "Epoch 201/1000, Loss: 11.0696\n",
      "Epoch 221/1000, Loss: 8.6528\n",
      "Epoch 241/1000, Loss: 3.2399\n",
      "Epoch 261/1000, Loss: 2.0430\n",
      "Epoch 281/1000, Loss: 1.1421\n",
      "Epoch 301/1000, Loss: 1.5472\n",
      "Epoch 321/1000, Loss: 0.6752\n",
      "Epoch 341/1000, Loss: 0.3619\n",
      "Epoch 361/1000, Loss: 0.2605\n",
      "Epoch 381/1000, Loss: 0.2012\n",
      "Epoch 401/1000, Loss: 0.1462\n",
      "Epoch 421/1000, Loss: 0.1031\n",
      "Epoch 441/1000, Loss: 0.0815\n",
      "Epoch 461/1000, Loss: 0.0588\n",
      "Epoch 481/1000, Loss: 0.0472\n",
      "Epoch 501/1000, Loss: 0.0343\n",
      "Epoch 521/1000, Loss: 0.0247\n",
      "Epoch 541/1000, Loss: 0.0180\n",
      "Epoch 561/1000, Loss: 0.0138\n",
      "Epoch 581/1000, Loss: 0.0106\n",
      "Epoch 601/1000, Loss: 0.0076\n",
      "Epoch 621/1000, Loss: 0.0057\n",
      "Epoch 641/1000, Loss: 0.0043\n",
      "Epoch 661/1000, Loss: 0.0029\n",
      "Epoch 681/1000, Loss: 0.0021\n",
      "Epoch 701/1000, Loss: 0.0017\n",
      "Epoch 721/1000, Loss: 0.0012\n",
      "Epoch 741/1000, Loss: 0.0010\n",
      "Epoch 761/1000, Loss: 0.0007\n",
      "Epoch 781/1000, Loss: 0.0005\n",
      "Epoch 801/1000, Loss: 0.0004\n",
      "Epoch 821/1000, Loss: 0.0003\n",
      "Epoch 841/1000, Loss: 0.0002\n",
      "Epoch 861/1000, Loss: 0.0002\n",
      "Epoch 881/1000, Loss: 0.0001\n",
      "Epoch 901/1000, Loss: 0.0001\n",
      "Epoch 921/1000, Loss: 0.0001\n",
      "Epoch 941/1000, Loss: 0.0000\n",
      "Epoch 961/1000, Loss: 0.0000\n",
      "Epoch 981/1000, Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "loss_lst = []\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for center, context in loader:\n",
    "        center = center.long()\n",
    "        context = context.long()\n",
    "\n",
    "        negative_samples = []\n",
    "        for c, o in zip(center, context):\n",
    "            negs = create_negative_sampling(\n",
    "                center_w=c.item(),\n",
    "                context_w=o.item(),\n",
    "                word_size=word_size,          \n",
    "                negative_nums=negative_nums,\n",
    "                indexed_set=indexed_set\n",
    "            )\n",
    "            negative_samples.append(negs)   \n",
    "\n",
    "        negative_samples = torch.LongTensor(negative_samples)\n",
    "\n",
    "        loss = model(center, context, negative_samples)\n",
    "\n",
    "        optimizer.zero_grad()   \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    if epoch % 20 == 0:\n",
    "        loss_lst.append(total_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef5a1626-2ac9-4cc8-a6f6-f43f5815610c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_similar_words(word: str, word_to_idx: dict, idx_to_word: dict, model, top_n: int = 5):\n",
    "    E_in  = model.Wv_embedding.weight.detach()\n",
    "    E_out = model.Wu_embedding.weight.detach()\n",
    "    E = (E_in + E_out) / 2    # [V, D]\n",
    "    E_norm = F.normalize(E, p=2, dim=1)\n",
    "    if word not in word_to_idx:\n",
    "        raise KeyError(f\"'{word}' does not exist\")\n",
    "    idx = word_to_idx[word]\n",
    "    v = E_norm[idx]                \n",
    "    sims = (E_norm @ v)             \n",
    "    sims[idx] = -1.0               \n",
    "    vals, ids = torch.topk(sims, k=top_n)\n",
    "    return [idx_to_word[i.item()] for i in ids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c1a7cb4-1814-4589-8ad1-e13f7da46d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pale', 'was', 'my', 'chance.', 'size']\n"
     ]
    }
   ],
   "source": [
    "similar = get_similar_words(\"sky\", word_to_idx, idx_to_word, model, top_n=5)\n",
    "print(similar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7bf1850e-12d0-45c6-ae7b-108fe35e13aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"sgns_model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38a48e3-ad58-4abd-9871-2a93fbc65fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
